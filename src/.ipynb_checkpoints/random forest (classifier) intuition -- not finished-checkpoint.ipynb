{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mlfromscratch.unsupervised_learning import PCA\n",
    "from mlfromscratch.supervised_learning import ClassificationTree\n",
    "from mlfromscratch.utils.misc import bar_widgets\n",
    "from mlfromscratch.utils import Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entropy  \n",
    "*  info (Chinese) https://www.cnblogs.com/ShaneZhang/p/3970176.html\n",
    "* https://medium.com/udacity/shannon-entropy-information-gain-and-picking-balls-from-buckets-5810d35d54b4"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attachment:image.png\"  width=\"250\" align=\"left\">     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H(x) = E[I(xi)] = E[ log(2,1/p(xi)) ] = -∑p(xi)log(2,p(xi)) (i=1,2,..n)  \n",
    "e.g. 32 instance: -(p1*log(2,p1) + p2 * log(2,p2) +　．．．　+p32 *log(2,p32))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 17.85 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000000 loops, best of 3: 281 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "math.log(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 14.86 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000000 loops, best of 3: 401 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "math.log(100) / math.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    \"\"\" Calculate the entropy of label array y \"\"\"\n",
    "    #log2 = lambda x: math.log(x) / math.log(2)\n",
    "    unique_labels = np.unique(y)\n",
    "    \n",
    "    entropy = 0\n",
    "    for label in unique_labels:\n",
    "        count = len(y[y == label])\n",
    "        p = count / len(y)\n",
    "        #entropy += -p * log2(p)\n",
    "        entropy += -p * math.log(p,2)\n",
    "        \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4464393446710155"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_entropy(np.array([1,1,1,2,5,3,7,7,5,12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide dataset based on if sample value on feature index is larger than the given threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide_on_feature(X, feature_i, threshold):\n",
    "    \"\"\" Divide dataset based on if sample value on feature index is larger than\n",
    "        the given threshold \"\"\"\n",
    "    \n",
    "    split_func = None\n",
    "    \n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        \n",
    "        split_func = lambda sample: sample[feature_i] >= threshold\n",
    "    else:\n",
    "        split_func = lambda sample: sample[feature_i] == threshold\n",
    "\n",
    "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
    "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
    "\n",
    "    return np.array([X_1, X_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,\n",
       "        3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,\n",
       "        8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,\n",
       "        3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,\n",
       "        1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first record\n",
    "X[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* e.g. for the feature index 2 (third column), split dataset to a -- feature value >=100, b -- feature value <100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((165, 30), (404, 30))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = divide_on_feature(X, 2, 100)\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_data(X, y, seed=None):\n",
    "    \"\"\" Random shuffle of the samples in X and y \"\"\"\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.5, shuffle=True, seed=None):\n",
    "    \"\"\" Split the data into train and test sets \"\"\"\n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y, seed)\n",
    "    # Split the training data from test data in the ratio specified in\n",
    "    # test_size\n",
    "    split_i = len(y) - int(len(y) // (1 / test_size))\n",
    "    X_train, X_test = X[:split_i], X[split_i:]\n",
    "    y_train, y_test = y[:split_i], y[split_i:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return random subsets (with replacements) of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_subsets(X, y, n_subsets, replacements=True):\n",
    "    \"\"\" Return random subsets (with replacements) of the data \"\"\"\n",
    "    n_samples = np.shape(X)[0]\n",
    "    # Concatenate x and y and do a random shuffle\n",
    "    X_y = np.concatenate((X, y.reshape((1, len(y))).T), axis=1)\n",
    "    np.random.shuffle(X_y)\n",
    "    subsets = []\n",
    "\n",
    "    # Uses 50% of training samples without replacements\n",
    "    subsample_size = int(n_samples // 2)\n",
    "    if replacements:\n",
    "        subsample_size = n_samples      # 100% with replacements\n",
    "\n",
    "    for _ in range(n_subsets):\n",
    "        idx = np.random.choice(\n",
    "            range(n_samples),\n",
    "            size=np.shape(range(subsample_size)),\n",
    "            replace=replacements)\n",
    "        X = X_y[idx][:, :-1]\n",
    "        y = X_y[idx][:, -1]\n",
    "        subsets.append([X, y])\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalize array\n",
    "http://www.fundza.com/vectors/normalize/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o normalize a vector in math means to divide each of its elements  \n",
    "to some value V so that the length/norm of the resulting vector is 1.  \n",
    "Turns out the needed V is equal the length (the length of the vector).  \n",
    "\n",
    "Say you have this array.  \n",
    "\n",
    "[-3, +4]  \n",
    "\n",
    "Its length (in Euclid metric) is: V = sqrt((-3)^2 + (+4)^2) = 5  \n",
    "\n",
    "So its corresponding normalized vector is:  \n",
    "\n",
    "[-3/5, +4/5]  \n",
    "\n",
    "Its length is now: sqrt ( (-3/5)^2 + (+4/5)^2 ) which is 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print np.array([[1,2,3],[4,5,6]]).shape\n",
    "np.array([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1],\n",
       "        [2],\n",
       "        [3]],\n",
       "\n",
       "       [[4],\n",
       "        [5],\n",
       "        [6]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print np.expand_dims(np.array([[1,2,3],[4,5,6]]), -1).shape\n",
    "np.expand_dims(np.array([[1,2,3],[4,5,6]]), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(X, axis=-1, order=2):\n",
    "    \"\"\" Normalize the dataset X \"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(X, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return X / np.expand_dims(l2, axis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  X_std = (X - X.mean(axis=0)) / X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    \"\"\" Standardize the dataset X \"\"\"\n",
    "    X_std = X\n",
    "    mean = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    for col in range(np.shape(X)[1]):\n",
    "        if std[col]:\n",
    "            X_std[:, col] = (X_std[:, col] - mean[col]) / std[col]\n",
    "    # X_std = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    return X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
    "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc_sc(y, y_pd):\n",
    "    n=len(y)\n",
    "    assert n==len(y_pd)\n",
    "    true = sum(map(lambda x,y: x==y, y, y_pd))\n",
    "    return float(true)/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* bagging and boosting  \n",
    "https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \"\"\"Random Forest classifier. Uses a collection of classification trees that\n",
    "    trains on random subsets of the data using a random subsets of the features.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_estimators: int\n",
    "        The number of classification trees that are used.\n",
    "    max_features: int\n",
    "        The maximum number of features that the classification trees are allowed to\n",
    "        use.\n",
    "    min_samples_split: int\n",
    "        The minimum number of samples needed to make a split when building a tree.\n",
    "    min_gain: float\n",
    "        The minimum impurity required to split the tree further. \n",
    "    max_depth: int\n",
    "        The maximum depth of a tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=100, max_features=None, min_samples_split=2,\n",
    "                 min_gain=0, max_depth=float(\"inf\")):\n",
    "        self.n_estimators = n_estimators    # Number of trees\n",
    "        self.max_features = max_features    # Maxmimum number of features per tree\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_gain = min_gain            # Minimum information gain req. to continue\n",
    "        self.max_depth = max_depth          # Maximum depth for tree\n",
    "        self.progressbar = progressbar.ProgressBar(widgets=bar_widgets)\n",
    "\n",
    "        # Initialize decision trees\n",
    "        self.trees = []\n",
    "        for _ in range(n_estimators):\n",
    "            self.trees.append(\n",
    "                ClassificationTree(\n",
    "                    min_samples_split=self.min_samples_split,\n",
    "                    min_impurity=min_gain,\n",
    "                    max_depth=self.max_depth))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = np.shape(X)[1]\n",
    "        # If max_features have not been defined => select it as\n",
    "        # sqrt(n_features)\n",
    "        if not self.max_features:\n",
    "            self.max_features = int(math.sqrt(n_features))\n",
    "\n",
    "        # Choose one random subset of the data for each tree\n",
    "        subsets = get_random_subsets(X, y, self.n_estimators)\n",
    "\n",
    "        for i in self.progressbar(range(self.n_estimators)):\n",
    "            X_subset, y_subset = subsets[i]\n",
    "            # Feature bagging (select random subsets of the features)\n",
    "            idx = np.random.choice(range(n_features), size=self.max_features, replace=True)\n",
    "            # Save the indices of the features for prediction\n",
    "            self.trees[i].feature_indices = idx\n",
    "            # Choose the features corresponding to the indices\n",
    "            X_subset = X_subset[:, idx]\n",
    "            # Fit the tree to the data\n",
    "            self.trees[i].fit(X_subset, y_subset)\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_preds = np.empty((X.shape[0], len(self.trees)))\n",
    "        # Let each tree make a prediction on the data\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            # Indices of the features that the tree has trained on\n",
    "            idx = tree.feature_indices\n",
    "            # Make a prediction based on those features\n",
    "            prediction = tree.predict(X[:, idx])\n",
    "            y_preds[:, i] = prediction\n",
    "            \n",
    "        y_pred = []\n",
    "        # For each sample\n",
    "        for sample_predictions in y_preds:\n",
    "            # Select the most common class prediction\n",
    "            y_pred.append(np.bincount(sample_predictions.astype('int')).argmax())\n",
    "        return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
